---

title: Transformer model

keywords: fastai
sidebar: home_sidebar

summary: "inspired from DETR : https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb#scrollTo=h91rsIPl7tVl"
description: "inspired from DETR : https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb#scrollTo=h91rsIPl7tVl"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/02_models.transformer.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>GeForce RTX 2070 SUPER
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Encoder">Encoder<a class="anchor-link" href="#Encoder"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Encoder" class="doc_header"><code>class</code> <code>Encoder</code><a href="https://github.com/tcapelle/moving_mnist/tree/master/moving_mnist/models/conv_rnn.py#L81" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Encoder</code>(<strong><code>n_in</code></strong>=<em><code>1</code></em>, <strong><code>szs</code></strong>=<em><code>[16, 64, 96]</code></em>, <strong><code>ks</code></strong>=<em><code>3</code></em>, <strong><code>rnn_ks</code></strong>=<em><code>5</code></em>, <strong><code>act</code></strong>=<em><code>'ReLU'</code></em>, <strong><code>norm</code></strong>=<em><code>None</code></em>, <strong><code>debug</code></strong>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use any torchvision architecture model (resnet, vgg, inception, etc...)</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">r34_encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This model encodes an image to a 512 feature space:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">r34_encoder</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([8, 512, 4, 4])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We recover a Tensor that has <code>512</code> channels and <code>(4,4)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="DTERDemo">DTERDemo<a class="anchor-link" href="#DTERDemo"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">DETRdemo</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Demo DETR implementation.</span>

<span class="sd">    Demo implementation of DETR in minimal number of lines, with the</span>
<span class="sd">    following differences wrt DETR in the paper:</span>
<span class="sd">    * learned positional encoding (instead of sine)</span>
<span class="sd">    * positional encoding is passed at input (instead of attention)</span>
<span class="sd">    * fc bbox predictor (instead of MLP)</span>
<span class="sd">    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.</span>
<span class="sd">    Only batch size 1 supported.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">nheads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                 <span class="n">num_encoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">num_decoder_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">debug</span> <span class="o">=</span> <span class="n">debug</span>
        <span class="c1"># create ResNet-50 backbone</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">()</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">fc</span>

        <span class="c1"># create conversion layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># create a default PyTorch transformer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Transformer</span><span class="p">(</span>
            <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">nheads</span><span class="p">,</span> <span class="n">num_encoder_layers</span><span class="p">,</span> <span class="n">num_decoder_layers</span><span class="p">)</span>

        <span class="c1"># prediction heads, one extra class for predicting non-empty slots</span>
        <span class="c1"># note that in baseline DETR linear_bbox layer is 3-layer MLP</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_class</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear_bbox</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

        <span class="c1"># output positional encodings (object queries)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">query_pos</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>

        <span class="c1"># spatial positional encodings</span>
        <span class="c1"># note that in baseline DETR we use sine positional encodings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">row_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">col_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># propagate inputs through ResNet-50 up to avg-pool layer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">maxpool</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">layer2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">layer3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">layer4</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># convert from 2048 to 256 feature planes for the transformer</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># construct positional encodings</span>
        <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">col_embed</span><span class="p">[:</span><span class="n">W</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">row_embed</span><span class="p">[:</span><span class="n">H</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;pos: </span><span class="si">{</span><span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        
        <span class="n">tf_input</span> <span class="o">=</span> <span class="n">pos</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">h</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;tf_input: </span><span class="si">{</span><span class="n">tf_input</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="c1"># propagate through the transformer</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span><span class="n">tf_input</span><span class="p">,</span>
                             <span class="bp">self</span><span class="o">.</span><span class="n">query_pos</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;tf_out: </span><span class="si">{</span><span class="n">h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            
        <span class="c1"># finally project transformer outputs to class labels and bounding boxes</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;pred_logits&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_class</span><span class="p">(</span><span class="n">h</span><span class="p">),</span> 
                <span class="s1">&#39;pred_boxes&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_bbox</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">demo</span> <span class="o">=</span> <span class="n">DETRdemo</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">demo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>pos: torch.Size([16, 1, 256])
tf_input: torch.Size([16, 1, 256])
tf_out: torch.Size([1, 100, 256])
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;pred_logits&#39;: tensor([[[ 0.3864, -0.2979,  1.3025,  ..., -0.5235, -0.2826,  0.1175],
          [-0.4414, -0.5061,  1.1318,  ..., -0.4527, -0.2506,  0.5684],
          [ 0.0783, -0.2800,  1.1213,  ..., -0.4288, -0.2279,  0.2643],
          ...,
          [-0.0739, -0.3486,  1.3377,  ..., -0.9759, -0.1284,  0.9343],
          [ 0.2017, -0.4327,  1.2409,  ..., -0.9430, -0.5388,  0.3167],
          [ 0.1249, -0.1855,  1.2717,  ..., -0.5588, -0.1075,  0.5636]]],
        grad_fn=&lt;AddBackward0&gt;),
 &#39;pred_boxes&#39;: tensor([[[0.3038, 0.5805, 0.6628, 0.3024],
          [0.3248, 0.4435, 0.7345, 0.3337],
          [0.3113, 0.5762, 0.7161, 0.3430],
          [0.4082, 0.5398, 0.5523, 0.3664],
          [0.3668, 0.4667, 0.6369, 0.4547],
          [0.3351, 0.4824, 0.6206, 0.5372],
          [0.3091, 0.4184, 0.6198, 0.3836],
          [0.4208, 0.4537, 0.5953, 0.4685],
          [0.3357, 0.4424, 0.6408, 0.4808],
          [0.2996, 0.3632, 0.6173, 0.3693],
          [0.2764, 0.4800, 0.7029, 0.3758],
          [0.3458, 0.4668, 0.7318, 0.4128],
          [0.2609, 0.3919, 0.5137, 0.4172],
          [0.2500, 0.5241, 0.6348, 0.3410],
          [0.2846, 0.4986, 0.5780, 0.3823],
          [0.2770, 0.5259, 0.6209, 0.3442],
          [0.3006, 0.5530, 0.6618, 0.4031],
          [0.3386, 0.5059, 0.6492, 0.4741],
          [0.3123, 0.5545, 0.6533, 0.5209],
          [0.3123, 0.4557, 0.5973, 0.3541],
          [0.3332, 0.3905, 0.6070, 0.4866],
          [0.3309, 0.3637, 0.6748, 0.4445],
          [0.2699, 0.4640, 0.6747, 0.2776],
          [0.2812, 0.4390, 0.6393, 0.3809],
          [0.3580, 0.4149, 0.6355, 0.3698],
          [0.3466, 0.4499, 0.5396, 0.3077],
          [0.3916, 0.4030, 0.6684, 0.4184],
          [0.1960, 0.5095, 0.6071, 0.4757],
          [0.3575, 0.4487, 0.6357, 0.3676],
          [0.2912, 0.4293, 0.6866, 0.4244],
          [0.4898, 0.5286, 0.7331, 0.3846],
          [0.3467, 0.4544, 0.5889, 0.4718],
          [0.3330, 0.4916, 0.6463, 0.3873],
          [0.3165, 0.5006, 0.6530, 0.4418],
          [0.2839, 0.4779, 0.6528, 0.3769],
          [0.2866, 0.4657, 0.6343, 0.4416],
          [0.3006, 0.5127, 0.6861, 0.4101],
          [0.2863, 0.4728, 0.5867, 0.4234],
          [0.3083, 0.4656, 0.5655, 0.3912],
          [0.2389, 0.5419, 0.6310, 0.3466],
          [0.3360, 0.4772, 0.6159, 0.4273],
          [0.3196, 0.5195, 0.6023, 0.4446],
          [0.3586, 0.5273, 0.6641, 0.3790],
          [0.3731, 0.4615, 0.6017, 0.4023],
          [0.4450, 0.4967, 0.5792, 0.4956],
          [0.2903, 0.3654, 0.6733, 0.4380],
          [0.2480, 0.5727, 0.5586, 0.3998],
          [0.3571, 0.4290, 0.6931, 0.4295],
          [0.3834, 0.4950, 0.4796, 0.4204],
          [0.3913, 0.5648, 0.6601, 0.3467],
          [0.3508, 0.5479, 0.6483, 0.4064],
          [0.2920, 0.5177, 0.6112, 0.4402],
          [0.4250, 0.4670, 0.6288, 0.3328],
          [0.3513, 0.4411, 0.6190, 0.3660],
          [0.3451, 0.5522, 0.5350, 0.4158],
          [0.3702, 0.3923, 0.5839, 0.4086],
          [0.3197, 0.4084, 0.6243, 0.3272],
          [0.3757, 0.5275, 0.6208, 0.4743],
          [0.3917, 0.4485, 0.6737, 0.3854],
          [0.2796, 0.4122, 0.6562, 0.3471],
          [0.3190, 0.3583, 0.6120, 0.2809],
          [0.4143, 0.4578, 0.5595, 0.4887],
          [0.3603, 0.5060, 0.6127, 0.3582],
          [0.3560, 0.5308, 0.6069, 0.3854],
          [0.3491, 0.5382, 0.5478, 0.5092],
          [0.3369, 0.4772, 0.6293, 0.5015],
          [0.3856, 0.4886, 0.6079, 0.3763],
          [0.3208, 0.5190, 0.5366, 0.3299],
          [0.3173, 0.4724, 0.5758, 0.3280],
          [0.2609, 0.4934, 0.6336, 0.4774],
          [0.2684, 0.4484, 0.6825, 0.4357],
          [0.2843, 0.4393, 0.6172, 0.3812],
          [0.3381, 0.4616, 0.6400, 0.4278],
          [0.2607, 0.5313, 0.6807, 0.4063],
          [0.3417, 0.4485, 0.5088, 0.4546],
          [0.3238, 0.4868, 0.5800, 0.4341],
          [0.2753, 0.4183, 0.6427, 0.3875],
          [0.3032, 0.5341, 0.6339, 0.3633],
          [0.3531, 0.4781, 0.6868, 0.3442],
          [0.3157, 0.4325, 0.5747, 0.4408],
          [0.2943, 0.5238, 0.4821, 0.5557],
          [0.2844, 0.4757, 0.6273, 0.4489],
          [0.2685, 0.4324, 0.5710, 0.4178],
          [0.3451, 0.4622, 0.6541, 0.4155],
          [0.3432, 0.4287, 0.6559, 0.4199],
          [0.4192, 0.5428, 0.5595, 0.4666],
          [0.3676, 0.5320, 0.5612, 0.4427],
          [0.2801, 0.4637, 0.7073, 0.3818],
          [0.2890, 0.4660, 0.6463, 0.3902],
          [0.3873, 0.4014, 0.6583, 0.3190],
          [0.3309, 0.5640, 0.5417, 0.3598],
          [0.4633, 0.5495, 0.6263, 0.4932],
          [0.2742, 0.5211, 0.5765, 0.3822],
          [0.3894, 0.4888, 0.5615, 0.4447],
          [0.3049, 0.4655, 0.6197, 0.4312],
          [0.4482, 0.5522, 0.6700, 0.4743],
          [0.2713, 0.5345, 0.5796, 0.4370],
          [0.3789, 0.4008, 0.6648, 0.4488],
          [0.2870, 0.4582, 0.6221, 0.3944],
          [0.2966, 0.5449, 0.7050, 0.4477]]], grad_fn=&lt;SigmoidBackward&gt;)}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Transformer-Model">Transformer Model<a class="anchor-link" href="#Transformer-Model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will try an architecture with an Encoder/Decoder model provided by the Transformer, instead of the ConvGRU layer.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="DETR" class="doc_header"><code>class</code> <code>DETR</code><a href="https://github.com/tcapelle/moving_mnist/tree/master/moving_mnist/models/transformer.py#L24" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>DETR</code>(<strong><code>n_in</code></strong>=<em><code>1</code></em>, <strong><code>n_out</code></strong>=<em><code>1</code></em>, <strong><code>hidden_dim</code></strong>=<em><code>256</code></em>, <strong><code>nheads</code></strong>=<em><code>8</code></em>, <strong><code>num_encoder_layers</code></strong>=<em><code>6</code></em>, <strong><code>num_decoder_layers</code></strong>=<em><code>6</code></em>, <strong><code>debug</code></strong>=<em><code>False</code></em>) :: <code>Module</code></p>
</blockquote>
<p>Same as <code>nn.Module</code>, but no need for subclasses to call <code>super().__init__</code></p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">detr</span> <span class="o">=</span> <span class="n">DETR</span><span class="p">(</span><span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># h = torch.rand(1, 5, 256, 4, 4)</span>

<span class="c1"># h.permute(0,2,1,3,4).flatten(2).permute(2,0,1).shape</span>

<span class="c1"># H, W = h.shape[-2:]</span>
<span class="c1"># T = h.shape[1]</span>

<span class="c1"># h.flatten(3).shape</span>

<span class="c1"># T</span>

<span class="c1"># detr.time_embed[:T].view(T,1,1,-1).repeat(1, H, W, 1).shape</span>

<span class="c1"># detr.col_embed[:W].view(1,1,W,-1).repeat(T, H, 1, 1).shape</span>

<span class="c1"># detr.row_embed[:H].view(1,H,1,-1).repeat(T, 1, W, 1).shape</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">detr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">128</span><span class="p">))</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>backbone: torch.Size([1, 5, 512, 4, 4])
h: torch.Size([1, 5, 256, 4, 4])
pos: torch.Size([80, 1, 256])
tf_input: torch.Size([80, 1, 256])
tf_out: torch.Size([1, 100, 256])
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 5, 1, 128, 128])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

