{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "> Encoder/Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *\n",
    "from fastai.text.models.awdlstm import RNNDropout\n",
    "from moving_mnist.models.conv_rnn import TimeDistributed, StackUnstack, StackLoss\n",
    "from moving_mnist.models.phy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(1)\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jhhuang96/ConvLSTM-PyTorch/blob/master/ConvRNN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a GRU cell the outputs and hidden are the same, last output must be equal to last hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvGRUCell(Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=(3,3), bias=True, activation=F.tanh, batchnorm=False):\n",
    "        \"\"\"\n",
    "        Initialize ConvGRU cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "        self.input_dim          = input_dim\n",
    "        self.hidden_dim         = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, (tuple, list)) else [kernel_size]*2\n",
    "        self.padding     = self.kernel_size[0] // 2, self.kernel_size[1] // 2\n",
    "        self.bias        = bias\n",
    "        self.activation  = activation\n",
    "        self.batchnorm   = batchnorm\n",
    "\n",
    "\n",
    "        self.conv_zr = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=2 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "        self.conv_h1 = nn.Conv2d(in_channels=self.input_dim,\n",
    "                              out_channels=self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "        self.conv_h2 = nn.Conv2d(in_channels=self.hidden_dim,\n",
    "                              out_channels=self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, input, h_prev=None):\n",
    "        #init hidden on forward\n",
    "        if h_prev is None:\n",
    "            h_prev = self.init_hidden(input)\n",
    "            \n",
    "        combined = torch.cat((input, h_prev), dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = F.sigmoid(self.conv_zr(combined))\n",
    "\n",
    "        z, r = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "\n",
    "        h_ = self.activation(self.conv_h1(input) + r * self.conv_h2(h_prev))\n",
    "\n",
    "        h_cur = (1 - z) * h_ + z * h_prev\n",
    "\n",
    "        return h_cur\n",
    "    \n",
    "    def init_hidden(self, input): \n",
    "        bs, ch, h, w = input.shape\n",
    "        return one_param(self).new_zeros(bs, self.hidden_dim, h, w)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #self.conv.reset_parameters()\n",
    "        nn.init.xavier_uniform_(self.conv_zr.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        self.conv_zr.bias.data.zero_()\n",
    "        nn.init.xavier_uniform_(self.conv_h1.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        self.conv_h1.bias.data.zero_()\n",
    "        nn.init.xavier_uniform_(self.conv_h2.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        self.conv_h2.bias.data.zero_()\n",
    "\n",
    "        if self.batchnorm:\n",
    "            self.bn1.reset_parameters()\n",
    "            self.bn2.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgru_cell = ConvGRUCell(16, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cgru_cell(torch.rand(1, 16, 16, 16)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, n_layers, batch_first=True, \n",
    "                 bias=True, activation=F.tanh, input_p=0.2, hidden_p=0.1, batchnorm=False):\n",
    "        super(ConvGRU, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, n_layers)\n",
    "        hidden_dim  = self._extend_for_multilayer(hidden_dim, n_layers)\n",
    "        activation  = self._extend_for_multilayer(activation, n_layers)\n",
    "\n",
    "        if not len(kernel_size) == len(hidden_dim) == len(activation) == n_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.input_p = input_p\n",
    "        self.hidden_p = hidden_p\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(self.n_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n",
    "\n",
    "            cell_list.append(ConvGRUCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias,\n",
    "                                          activation=activation[i],\n",
    "                                          batchnorm=batchnorm))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([nn.Dropout(hidden_p) for l in range(n_layers)])\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def __repr__(self): \n",
    "        s = f'ConvGru(in={self.input_dim}, out={self.hidden_dim[0]}, ks={self.kernel_size[0]}, '\n",
    "        s += f'n_layers={self.n_layers}, input_p={self.input_p}, hidden_p={self.hidden_p})'\n",
    "        return s\n",
    "    def forward(self, input, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor:\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state:\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        input = self.input_dp(input)\n",
    "        cur_layer_input = torch.unbind(input, dim=int(self.batch_first))\n",
    "        \n",
    "        if hidden_state is None:\n",
    "            hidden_state = self.get_init_states(cur_layer_input[0])\n",
    "\n",
    "        seq_len = len(cur_layer_input)\n",
    "        last_state_list   = []\n",
    "        \n",
    "        for l, (gru_cell, hid_dp) in enumerate(zip(self.cell_list, self.hidden_dps)):\n",
    "            h = hidden_state[l]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h = gru_cell(input=cur_layer_input[t], h_prev=h)\n",
    "                output_inner.append(h)\n",
    "\n",
    "            cur_layer_input = torch.stack(output_inner)  #list to array\n",
    "            if l != self.n_layers: cur_layer_input = hid_dp(cur_layer_input)\n",
    "            last_state_list.append(h)\n",
    "\n",
    "        layer_output = torch.stack(output_inner, dim=int(self.batch_first))\n",
    "        last_state_list = torch.stack(last_state_list, dim=0)\n",
    "        return layer_output, last_state_list\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for c in self.cell_list:\n",
    "            c.reset_parameters()\n",
    "\n",
    "    def get_init_states(self, input):\n",
    "        init_states = []\n",
    "        for gru_cell in self.cell_list:\n",
    "            init_states.append(gru_cell.init_hidden(input))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or (isinstance(kernel_size, list)\n",
    "            and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgru = ConvGRU(16, 32, (3, 3), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvGru(in=16, out=32, ks=(3, 3), n_layers=2, input_p=0.2, hidden_p=0.1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cgru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output, last_state_list = cgru(torch.rand(8,10,16,6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 10, 32, 6, 6]), torch.Size([2, 8, 32, 6, 6]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output.shape, last_state_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output, last_state_list = cgru(torch.rand(8,10,16,6,6), last_state_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN image encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class dcgan_conv(nn.Sequential):\n",
    "    def __init__(self, nin, nout, stride):\n",
    "        layers = [nn.Conv2d(nin, nout, kernel_size=(3,3), stride=stride, padding=1),\n",
    "                  nn.GroupNorm(4,nout),\n",
    "                  nn.LeakyReLU(0.2, inplace=True)]\n",
    "        super().__init__(*layers)\n",
    "\n",
    "        \n",
    "class dcgan_upconv(nn.Sequential):\n",
    "    def __init__(self, nin, nout, stride):\n",
    "        layers = [nn.ConvTranspose2d(nin, nout,(3,3), stride=stride,\n",
    "                                   padding=1,output_padding=1 if stride==2 else 0),\n",
    "                  nn.GroupNorm(4,nout),\n",
    "                  nn.LeakyReLU(0.2, inplace=True)]\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dcgan_conv(\n",
       "  (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): GroupNorm(4, 10, eps=1e-05, affine=True)\n",
       "  (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcgan_conv(1,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dcgan_upconv(\n",
       "  (0): ConvTranspose2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): GroupNorm(4, 10, eps=1e-05, affine=True)\n",
       "  (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcgan_upconv(1,10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class image_encoder(Module):\n",
    "    def __init__(self, nc=1):\n",
    "        nf = 16\n",
    "        # input is (nc) x 64 x 64\n",
    "        self.c1 = dcgan_conv(nc, nf//2, stride=1) # (nf) x 64 x 64\n",
    "        self.c2 = dcgan_conv(nf//2, nf, stride=1) # (nf) x 64 x 64\n",
    "        self.c3 = dcgan_conv(nf, nf*2, stride=2) # (2*nf) x 32 x 32\n",
    "        self.c4 = dcgan_conv(nf*2, nf*2, stride=1) # (2*nf) x 32 x 32\n",
    "        self.c5 = dcgan_conv(nf*2, nf*4, stride=2) # (4*nf) x 16 x 16\n",
    "        self.c6 = dcgan_conv(nf*4, nf*4, stride=1) # (4*nf) x 16 x 16          \n",
    "\n",
    "    def forward(self, input):\n",
    "        h1 = self.c1(input)  # (nf/2) x 64 x 64\n",
    "        h2 = self.c2(h1)     # (nf) x 64 x 64\n",
    "        h3 = self.c3(h2)     # (2*nf) x 32 x 32\n",
    "        h4 = self.c4(h3)     # (2*nf) x 32 x 32\n",
    "        h5 = self.c5(h4)     # (4*nf) x 16 x 16\n",
    "        h6 = self.c6(h5)     # (4*nf) x 16 x 16          \n",
    "        return h6 #[h1, h2, h3, h4, h5, h6]\n",
    "\n",
    "\n",
    "class image_decoder(Module):\n",
    "    def __init__(self, nc=1, nf=16):\n",
    "        self.upc1 = dcgan_upconv(nf*4, nf*4, stride=1) #(nf*4) x 16 x 16\n",
    "        self.upc2 = dcgan_upconv(nf*4, nf*2, stride=2) #(nf*2) x 32 x 32\n",
    "        self.upc3 = dcgan_upconv(nf*2, nf*2, stride=1) #(nf*2) x 32 x 32\n",
    "        self.upc4 = dcgan_upconv(nf*2, nf, stride=2)   #(nf) x 64 x 64\n",
    "        self.upc5 = dcgan_upconv(nf, nf, stride=1)   #(nf/2) x 64 x 64\n",
    "        self.upc6 = nn.ConvTranspose2d(in_channels=nf,out_channels=nc,kernel_size=(3,3),stride=1,padding=1)  #(nc) x 64 x 64\n",
    "\n",
    "    def forward(self, vec):  \n",
    "        d1 = self.upc1(vec)  #(nf*4) x 16 x 16\n",
    "        d2 = self.upc2(d1)   #(nf*2) x 32 x 32\n",
    "        d3 = self.upc3(d2)   #(nf*2) x 32 x 32\n",
    "        d4 = self.upc4(d3)   #(nf) x 64 x 64\n",
    "        d5 = self.upc5(d4)   #(nf/2) x 64 x 64\n",
    "        d6 = self.upc6(d5)   #(nc) x 64 x 64\n",
    "        return d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder = image_encoder()\n",
    "img_decoder = image_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_img = img_encoder(torch.rand(1,1,64,64))\n",
    "enc_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_decoder(enc_img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Seq2Seq(Module):\n",
    "    \"Simple seq2seq model\"\n",
    "    def __init__(self, seq_len=2, ch_out=1, ks=3, n_layers=1, debug=False):\n",
    "        store_attr()\n",
    "        self.img_encoder = TimeDistributed(image_encoder())\n",
    "        self.img_decoder = TimeDistributed(image_decoder(ch_out))\n",
    "        self.rnn = ConvGRU(64, 64, (ks, ks) if not isinstance(ks, tuple) else ks, n_layers)\n",
    "        self.pr = 0.0\n",
    "    def forward(self, x, targ=None):\n",
    "        if self.debug: print('pr: ', self.pr)\n",
    "        enc_imgs = self.img_encoder(x)\n",
    "        if self.debug: print('enc_imgs shape', enc_imgs.shape)\n",
    "        enc_outs, h = self.rnn(enc_imgs)\n",
    "        dec_imgs = self.img_decoder(enc_outs)\n",
    "        if targ is not None:\n",
    "            if self.debug: print('targ is not None')\n",
    "            enc_targs = self.img_encoder(targ) if targ is not None else None\n",
    "            dec_inp = enc_targs[:, [0], ...]\n",
    "        else:\n",
    "            dec_inp = enc_outs[:, [-1], ...].detach()\n",
    "        outs = [dec_imgs[:,[-1],...]]\n",
    "        if self.debug: print('initial out:',  outs[0].shape)\n",
    "        for i in range(self.seq_len-1):\n",
    "            dec_inp, h = self.rnn(dec_inp, h)\n",
    "            new_img = self.img_decoder(dec_inp)\n",
    "            outs.append(new_img)\n",
    "            if self.debug: print('i out:',  outs[i].shape)\n",
    "            if (targ is not None) and (random.random()<self.pr):\n",
    "                if self.debug: print('pr i:', i)\n",
    "                dec_inp = enc_targs[:,[i+1],:]\n",
    "        return torch.stack(outs, dim=1).squeeze(2), dec_imgs[:,:-1, ...], x[:,1:,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StackUnstack(Seq2Seq(5, 1, debug=True), dim=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_list = [torch.rand(2,1,64,64).cuda() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr:  0.0\n",
      "enc_imgs shape torch.Size([2, 5, 64, 16, 16])\n",
      "initial out: torch.Size([2, 1, 1, 64, 64])\n",
      "i out: torch.Size([2, 1, 1, 64, 64])\n",
      "i out: torch.Size([2, 1, 1, 64, 64])\n",
      "i out: torch.Size([2, 1, 1, 64, 64])\n",
      "i out: torch.Size([2, 1, 1, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " torch.Size([2, 1, 64, 64]),\n",
       " torch.Size([2, 1, 64, 64]),\n",
       " torch.Size([2, 1, 64, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, enc_imgs, enc_targets = m(imgs_list)\n",
    "assert len(enc_imgs)==len(enc_targets)\n",
    "len(out), out[0].shape, enc_imgs[0].shape, enc_targets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = StackLoss(MSELossFlat())\n",
    "loss = mse_loss(out, [torch.zeros_like(o) for o in out]) + mse_loss(enc_imgs, enc_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Seq2SeqPhy(Module):\n",
    "    \"Simple seq2seq model\"\n",
    "    def __init__(self, seq_len=2, ch_out=1, ks=3, n_layers=1, debug=False):\n",
    "        store_attr()\n",
    "        self.img_encoder = TimeDistributed(image_encoder())\n",
    "        self.img_decoder = TimeDistributed(image_decoder(ch_out))\n",
    "        self.rnn = ConvGRU(64, 64, (ks, ks) if not isinstance(ks, tuple) else ks, n_layers)\n",
    "        self.phy = PhyCell(64, [49], ks=7, n_layers=1)\n",
    "        self.pr = 0.0\n",
    "        \n",
    "    def forward(self, x, targ=None):\n",
    "        enc_imgs = self.img_encoder(x)\n",
    "        \n",
    "        #cells\n",
    "        enc_outs, h = self.rnn(enc_imgs)\n",
    "        phy_out, phy_h = self.phy(enc_imgs)\n",
    "        \n",
    "        dec_imgs = self.img_decoder(enc_outs) + self.img_decoder(phy_out)\n",
    "        \n",
    "        if targ is not None:\n",
    "            if self.debug: print('targ is not None')\n",
    "            enc_targs = self.img_encoder(targ) if targ is not None else None\n",
    "            dec_inp = enc_targs[:, [0], ...]\n",
    "            phy_inp = enc_targs[:, [0], ...]\n",
    "        else:\n",
    "            dec_inp = enc_outs[:, [-1], ...].detach()\n",
    "            phy_inp = phy_out[:,[-1],...].detach()\n",
    "                \n",
    "        outs = [dec_imgs[:,[-1],...]]\n",
    "\n",
    "        for i in range(self.seq_len-1):\n",
    "            dec_inp, h = self.rnn(dec_inp, h)\n",
    "            phy_inp, phy_h = self.phy(phy_inp, phy_h)\n",
    "            new_img = self.img_decoder(dec_inp) + self.img_decoder(phy_inp)\n",
    "            outs.append(new_img)\n",
    "            if (targ is not None) and (random.random()<self.pr):\n",
    "                dec_inp = enc_targs[:,[i+1],:]\n",
    "                phy_inp = enc_targs[:,[i+1],:]\n",
    "            else:\n",
    "                dec_inp = self.img_encoder(new_img)\n",
    "                phy_inp = self.img_encoder(new_img)\n",
    "        return torch.stack(outs, dim=1).squeeze(2), dec_imgs[:,:-1, ...], x[:,1:,...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full integration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = StackUnstack(Seq2SeqPhy(5, 1, debug=True), dim=1).cuda()\n",
    "m2.module.pr=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_list = [torch.rand(2,1,64,64).cuda() for _ in range(5)]\n",
    "img_targets = [torch.rand(2,1,64,64).cuda() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " torch.Size([2, 1, 64, 64]),\n",
       " torch.Size([2, 1, 64, 64]),\n",
       " torch.Size([2, 1, 64, 64]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, enc_imgs, enc_targets = m2(imgs_list)\n",
    "\n",
    "assert len(enc_imgs)==len(enc_targets)\n",
    "len(out), out[0].shape, enc_imgs[0].shape, enc_targets[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = StackLoss(MSELossFlat())\n",
    "loss = mse_loss(out, [torch.zeros(2,1,64,64).cuda() for o in out]) #+ mse_loss(enc_imgs, enc_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EncoderLoss(Callback):\n",
    "    def __init__(self, enc_loss_func, alpha=0.5):\n",
    "        store_attr()\n",
    "    def after_pred(self):\n",
    "        self.learn.pred, self.learn.encoder_y, self.learn.encoder_targ = self.pred\n",
    "    def after_loss(self):\n",
    "        self.learn.loss += self.alpha*self.enc_loss_func(self.learn.encoder_y, self.learn.encoder_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Forcing Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TeacherForcing(Callback):\n",
    "    def __init__(self, end_epoch):\n",
    "        self.end_epoch = end_epoch\n",
    "    def before_batch(self):\n",
    "        self.learn.xb = self.learn.xb + self.learn.yb\n",
    "    def before_epoch(self):\n",
    "        self.learn.model.module.pr = 1 - self.learn.epoch/self.end_epoch\n",
    "    def before_validate(self):\n",
    "        \"force forecasting\"\n",
    "        self.learn.model.module.pr = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_models.conv_rnn.ipynb.\n",
      "Converted 02_models.dcn.ipynb.\n",
      "Converted 02_models.transformer.ipynb.\n",
      "Converted 02_tcn.ipynb.\n",
      "Converted 03_phy.ipynb.\n",
      "Converted 04_seq2seq.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
