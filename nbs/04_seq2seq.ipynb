{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq\n",
    "> Encoder/Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.all import *\n",
    "from fastai.text.models.awdlstm import RNNDropout\n",
    "from moving_mnist.models.conv_rnn import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(1)\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jhhuang96/ConvLSTM-PyTorch/blob/master/ConvRNN.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a GRU cell the outputs and hidden are the same, last output must be equal to last hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvGRUCell(Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=(3,3), bias=True, activation=F.tanh, batchnorm=False):\n",
    "        \"\"\"\n",
    "        Initialize ConvGRU cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "        self.input_dim          = input_dim\n",
    "        self.hidden_dim         = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, (tuple, list)) else [kernel_size]*2\n",
    "        self.padding     = self.kernel_size[0] // 2, self.kernel_size[1] // 2\n",
    "        self.bias        = bias\n",
    "        self.activation  = activation\n",
    "        self.batchnorm   = batchnorm\n",
    "\n",
    "\n",
    "        self.conv_zr = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=2 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "        self.conv_h1 = nn.Conv2d(in_channels=self.input_dim,\n",
    "                              out_channels=self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "        self.conv_h2 = nn.Conv2d(in_channels=self.hidden_dim,\n",
    "                              out_channels=self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, input, h_prev=None):\n",
    "        #init hidden on forward\n",
    "        if h_prev is None:\n",
    "            h_prev = self.init_hidden(input)\n",
    "            \n",
    "        combined = torch.cat((input, h_prev), dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = F.sigmoid(self.conv_zr(combined))\n",
    "\n",
    "        z, r = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "\n",
    "        h_ = self.activation(self.conv_h1(input) + r * self.conv_h2(h_prev))\n",
    "\n",
    "        h_cur = (1 - z) * h_ + z * h_prev\n",
    "\n",
    "        return h_cur\n",
    "    \n",
    "    def init_hidden(self, input): \n",
    "        bs, ch, h, w = input.shape\n",
    "        return one_param(self).new_zeros(bs, self.hidden_dim, h, w)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        #self.conv.reset_parameters()\n",
    "        nn.init.xavier_uniform_(self.conv_zr.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        self.conv_zr.bias.data.zero_()\n",
    "        nn.init.xavier_uniform_(self.conv_h1.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        self.conv_h1.bias.data.zero_()\n",
    "        nn.init.xavier_uniform_(self.conv_h2.weight, gain=nn.init.calculate_gain('tanh'))\n",
    "        self.conv_h2.bias.data.zero_()\n",
    "\n",
    "        if self.batchnorm:\n",
    "            self.bn1.reset_parameters()\n",
    "            self.bn2.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgru_cell = ConvGRUCell(16, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cgru_cell(torch.rand(1, 16, 16, 16)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvGRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, n_layers, batch_first=True, \n",
    "                 bias=True, activation=F.tanh, input_p=0.2, hidden_p=0.1, batchnorm=False):\n",
    "        super(ConvGRU, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, n_layers)\n",
    "        hidden_dim  = self._extend_for_multilayer(hidden_dim, n_layers)\n",
    "        activation  = self._extend_for_multilayer(activation, n_layers)\n",
    "\n",
    "        if not len(kernel_size) == len(hidden_dim) == len(activation) == n_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim  = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.input_p = input_p\n",
    "        self.hidden_p = hidden_p\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(self.n_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n",
    "\n",
    "            cell_list.append(ConvGRUCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias,\n",
    "                                          activation=activation[i],\n",
    "                                          batchnorm=batchnorm))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([nn.Dropout(hidden_p) for l in range(n_layers)])\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def __repr__(self): \n",
    "        s = f'ConvGru(in={self.input_dim}, out={self.hidden_dim[0]}, ks={self.kernel_size[0]}, '\n",
    "        s += f'n_layers={self.n_layers}, input_p={self.input_p}, hidden_p={self.hidden_p})'\n",
    "        return s\n",
    "    def forward(self, input, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor:\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state:\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        input = self.input_dp(input)\n",
    "        cur_layer_input = torch.unbind(input, dim=int(self.batch_first))\n",
    "        \n",
    "        if hidden_state is None:\n",
    "            hidden_state = self.get_init_states(cur_layer_input[0])\n",
    "\n",
    "        seq_len = len(cur_layer_input)\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list   = []\n",
    "        \n",
    "        for l, (gru_cell, hid_dp) in enumerate(zip(self.cell_list, self.hidden_dps)):\n",
    "            h = hidden_state[l]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h = gru_cell(input=cur_layer_input[t], h_prev=h)\n",
    "                output_inner.append(h)\n",
    "\n",
    "            cur_layer_input = torch.stack(output_inner)  #list to array\n",
    "            if l != self.n_layers: cur_layer_input = hid_dp(cur_layer_input)\n",
    "            last_state_list.append(h)\n",
    "\n",
    "        layer_output = torch.stack(output_inner, dim=int(self.batch_first))\n",
    "        last_state_list = torch.stack(last_state_list, dim=0)\n",
    "        return layer_output, last_state_list\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for c in self.cell_list:\n",
    "            c.reset_parameters()\n",
    "\n",
    "    def get_init_states(self, input):\n",
    "        init_states = []\n",
    "        for gru_cell in self.cell_list:\n",
    "            init_states.append(gru_cell.init_hidden(input))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or (isinstance(kernel_size, list)\n",
    "            and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgru = ConvGRU(16, 32, (3, 3), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvGru(in=16, out=32, ks=(3, 3), n_layers=2, input_p=0.2, hidden_p=0.1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cgru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output, last_state_list = cgru(torch.rand(8,10,16,6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 32, 6, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 32, 6, 6])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_state_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_output, last_state_list = cgru(torch.rand(8,10,16,6,6), last_state_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN image encoder/decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class dcgan_conv(Module):\n",
    "    def __init__(self, nin, nout, stride):\n",
    "        self.main = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=nin, out_channels=nout, kernel_size=(3,3), stride=stride, padding=1),\n",
    "                nn.GroupNorm(4,nout),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "        \n",
    "class dcgan_upconv(Module):\n",
    "    def __init__(self, nin, nout, stride):\n",
    "        if (stride ==2):\n",
    "            output_padding = 1\n",
    "        else:\n",
    "            output_padding = 0\n",
    "        self.main = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=nin,out_channels=nout,kernel_size=(3,3), stride=stride,padding=1,output_padding=output_padding),\n",
    "                nn.GroupNorm(4,nout),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class image_encoder(Module):\n",
    "    def __init__(self, nc=1):\n",
    "        nf = 16\n",
    "        # input is (nc) x 64 x 64\n",
    "        self.c1 = dcgan_conv(nc, int(nf/2), stride=1) # (nf) x 64 x 64\n",
    "        self.c2 = dcgan_conv(int(nf/2), nf, stride=1) # (nf) x 64 x 64\n",
    "        self.c3 = dcgan_conv(nf, nf*2, stride=2) # (2*nf) x 32 x 32\n",
    "        self.c4 = dcgan_conv(nf*2, nf*2, stride=1) # (2*nf) x 32 x 32\n",
    "        self.c5 = dcgan_conv(nf*2, nf*4, stride=2) # (4*nf) x 16 x 16\n",
    "        self.c6 = dcgan_conv(nf*4, nf*4, stride=1) # (4*nf) x 16 x 16          \n",
    "\n",
    "    def forward(self, input):\n",
    "        h1 = self.c1(input)  # (nf/2) x 64 x 64\n",
    "        h2 = self.c2(h1)     # (nf) x 64 x 64\n",
    "        h3 = self.c3(h2)     # (2*nf) x 32 x 32\n",
    "        h4 = self.c4(h3)     # (2*nf) x 32 x 32\n",
    "        h5 = self.c5(h4)     # (4*nf) x 16 x 16\n",
    "        h6 = self.c6(h5)     # (4*nf) x 16 x 16          \n",
    "        return h6 #[h1, h2, h3, h4, h5, h6]\n",
    "\n",
    "\n",
    "class image_decoder(Module):\n",
    "    def __init__(self, nc=1):\n",
    "        nf = 16\n",
    "        self.upc1 = dcgan_upconv(nf*4, nf*4, stride=1) #(nf*4) x 16 x 16\n",
    "        self.upc2 = dcgan_upconv(nf*4, nf*2, stride=2) #(nf*2) x 32 x 32\n",
    "        self.upc3 = dcgan_upconv(nf*2, nf*2, stride=1) #(nf*2) x 32 x 32\n",
    "        self.upc4 = dcgan_upconv(nf*2, nf, stride=2)   #(nf) x 64 x 64\n",
    "        self.upc5 = dcgan_upconv(nf, nf, stride=1)   #(nf/2) x 64 x 64\n",
    "        self.upc6 = nn.ConvTranspose2d(in_channels=nf,out_channels=nc,kernel_size=(3,3),stride=1,padding=1)  #(nc) x 64 x 64\n",
    "\n",
    "    def forward(self, vec):  \n",
    "        d1 = self.upc1(vec)  #(nf*4) x 16 x 16\n",
    "        d2 = self.upc2(d1)   #(nf*2) x 32 x 32\n",
    "        d3 = self.upc3(d2)   #(nf*2) x 32 x 32\n",
    "        d4 = self.upc4(d3)   #(nf) x 64 x 64\n",
    "        d5 = self.upc5(d4)   #(nf/2) x 64 x 64\n",
    "        d6 = self.upc6(d5)   #(nc) x 64 x 64\n",
    "        return d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder = image_encoder()\n",
    "img_decoder = image_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_img = img_encoder(torch.rand(1,1,64,64))\n",
    "enc_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_decoder(enc_img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Encoder(Module):\n",
    "    def __init__(self, ks=3, n_layers=1, debug=False):\n",
    "        self.debug = debug\n",
    "        self.img_encoder = TimeDistributed(image_encoder())\n",
    "        self.cgru = ConvGRU(64, 64, (ks, ks) if not isinstance(ks, tuple) else ks, n_layers)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"inputs.shape bs,seq_len,1,64,64\"\n",
    "        encoded_images = self.img_encoder(inputs)\n",
    "        if self.debug: print(f'encoded_images: {encoded_images.shape}')\n",
    "        out, hidden = self.cgru(encoded_images)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.rand(2, 10, 1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_images: torch.Size([2, 10, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "enc_outs, h = enc(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 64, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 64, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Decoder(Module):\n",
    "    def __init__(self, ch_out=1, ks=3, n_layers=1, debug=False):\n",
    "        self.debug = debug\n",
    "        self.img_decoder = TimeDistributed(image_decoder(ch_out))\n",
    "        self.cgru = ConvGRU(64, 64, (ks, ks) if not isinstance(ks, tuple) else ks, n_layers)\n",
    "    \n",
    "    def forward(self, dec_inp, h):\n",
    "        if self.debug: print(f'dec_inp: {dec_inp.shape}')\n",
    "        out, hid = self.cgru(dec_inp, h)\n",
    "        if self.debug: print(f'out: {out.shape}')\n",
    "        return self.img_decoder(out), out, hid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = Decoder(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 64, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dec_inp: torch.Size([2, 1, 64, 16, 16])\n",
      "out: torch.Size([2, 1, 64, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 64, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_out, dec_out, dec_hid = dec(enc_outs[:, [-1],...], h)\n",
    "dec_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StackUnstack(Module):\n",
    "    \"Stack together inputs, apply module, unstack output\"\n",
    "    def __init__(self, module, dim=2):\n",
    "        self.dim = dim\n",
    "        self.module = module\n",
    "    def forward(self, *args):\n",
    "        input = [torch.stack(x, dim=self.dim) for x in args]\n",
    "        output = self.module(*input)\n",
    "        return output.unbind(dim=self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SimpleModel(Module):\n",
    "    \"Simple Encoder/Decoder module\"\n",
    "    def __init__(self, seq_len=2, ch_out=1):\n",
    "        store_attr()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(ch_out)\n",
    "        self.pr = 0.0\n",
    "    def forward(self, x, targ=None):\n",
    "        enc_outs, dec_h = self.encoder(x)\n",
    "        dec_inp = enc_outs[:, [-1], ...].detach()\n",
    "        outs = []\n",
    "        for i in range(self.seq_len):\n",
    "            img_out, dec_inp, dec_h = self.decoder(dec_inp, dec_h)\n",
    "            outs.append(img_out.squeeze(1))\n",
    "        \n",
    "        return torch.stack(outs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = StackUnstack(SimpleModel(5, 2), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_list = [torch.rand(2,1,64,64) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(imgs_list)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Forcing Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TeacherForcing(Callback):\n",
    "    def __init__(self, end_epoch):\n",
    "        self.end_epoch = end_epoch\n",
    "    def begin_batch(self):\n",
    "        self.learn.xb = self.learn.xb + self.learn.yb\n",
    "    def begin_epoch(self):\n",
    "        self.learn.model.pr = 1 - self.learn.epoch/self.end_epoch\n",
    "    def begin_validate(self):\n",
    "        \"force forecasting\"\n",
    "        self.learn.model.pr = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_models.conv_rnn.ipynb.\n",
      "Converted 02_models.dcn.ipynb.\n",
      "Converted 02_models.transformer.ipynb.\n",
      "Converted 02_tcn.ipynb.\n",
      "Converted 03_phy.ipynb.\n",
      "Converted 04_seq2seq.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
