{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model\n",
    "> inspired from DETR : https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb#scrollTo=h91rsIPl7tVl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.vision.all import *\n",
    "from moving_mnist.models.conv_rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(1)\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(create_cnn_model)\n",
    "class Encoder(Module):\n",
    "    def __init__(self, arch=resnet34, n_in=3, weights_file=None, n_out=1, strict=False, pretrained=False, **kwargs):\n",
    "        \"Encoder based on resnet, returns the feature map\"\n",
    "        model = create_cnn_model(arch, n_out=n_out, n_in=n_in, pretrained=pretrained, **kwargs)\n",
    "        if weights_file is not None: \n",
    "            load_res = load_model(weights_file, model, opt=None, strict=strict)\n",
    "            print(f'Loading model from file {weights_file} \\n>missing keys: {load_res}')\n",
    "        self.body = model[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use any torchvision architecture model (resnet, vgg, inception, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r34_encoder = Encoder(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model encodes an image to a 512 feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r34_encoder(torch.rand(8, 3, 128, 128)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recover a Tensor that has `512` channels and `(4,4)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTERDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRdemo(nn.Module):\n",
    "    \"\"\"\n",
    "    Demo DETR implementation.\n",
    "\n",
    "    Demo implementation of DETR in minimal number of lines, with the\n",
    "    following differences wrt DETR in the paper:\n",
    "    * learned positional encoding (instead of sine)\n",
    "    * positional encoding is passed at input (instead of attention)\n",
    "    * fc bbox predictor (instead of MLP)\n",
    "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
    "    Only batch size 1 supported.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, debug=False):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = resnet50()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads, one extra class for predicting non-empty slots\n",
    "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
    "        x = self.backbone.conv1(inputs)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "        if self.debug: print(f'pos: {pos.shape}')\n",
    "        \n",
    "        tf_input = pos + 0.1 * h.flatten(2).permute(2, 0, 1)\n",
    "        if self.debug: print(f'tf_input: {tf_input.shape}')\n",
    "        # propagate through the transformer\n",
    "        h = self.transformer(tf_input,\n",
    "                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
    "        if self.debug: print(f'tf_out: {h.shape}')\n",
    "            \n",
    "        # finally project transformer outputs to class labels and bounding boxes\n",
    "        return {'pred_logits': self.linear_class(h), \n",
    "                'pred_boxes': self.linear_bbox(h).sigmoid()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = DETRdemo(10, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: torch.Size([16, 1, 256])\n",
      "tf_input: torch.Size([16, 1, 256])\n",
      "tf_out: torch.Size([1, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "demo(torch.rand(1,3,128,128));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try an architecture with an Encoder/Decoder model provided by the Transformer, instead of the ConvGRU layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DETR(Module):\n",
    "    def __init__(self,  arch=resnet34, n=80, n_in=1, n_out=1, hidden_dim=256, nheads=4, num_encoder_layers=4, \n",
    "                 num_decoder_layers=4, debug=False):\n",
    "        self.debug = debug\n",
    "        \n",
    "        #the image encoder\n",
    "        self.backbone = TimeDistributed(Encoder(arch, n_in=n_in, pretrained=True))\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = TimeDistributed(nn.Conv2d(512, hidden_dim, 1))\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "        \n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.pos = nn.Parameter(torch.rand(n, hidden_dim))\n",
    "#         self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 4))\n",
    "#         self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 4))\n",
    "#         self.time_embed =nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        \n",
    "        #decoder\n",
    "        self.decoder = TimeDistributed(nn.Sequential(\n",
    "                           UpsampleBlock(256, 128, residual=False),\n",
    "                           UpsampleBlock(128, 128, residual=False),\n",
    "                           UpsampleBlock(128, 64, residual=False),\n",
    "                           UpsampleBlock(64, 32, residual=False),\n",
    "                           UpsampleBlock(32, 16, residual=False),\n",
    "                           nn.Conv2d(16, n_out, 3,1,1))\n",
    "                                      )\n",
    "        self.lin = nn.Linear(100,n)  #hardcodeed\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet up to avg-pool layer\n",
    "        x = self.backbone(inputs)\n",
    "        if self.debug: print(f'backbone: {x.shape}')\n",
    "            \n",
    "        # convert from the latent dim to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "        if self.debug: print(f'h: {h.shape}')\n",
    "            \n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        T = h.shape[1]\n",
    "#         pos = torch.cat([\n",
    "#             self.time_embed[:T].view(T,1,1,-1).repeat(1, H, W, 1),\n",
    "#             self.col_embed[:W].view(1,1,W,-1).repeat(T, H, 1, 1),\n",
    "#             self.row_embed[:H].view(1,H,1,-1).repeat(T, 1, W, 1),\n",
    "#         ], dim=-1).flatten(0, 2).unsqueeze(1)\n",
    "        pos = self.pos.unsqueeze(1)\n",
    "        if self.debug: print(f'pos: {pos.shape}')\n",
    "        \n",
    "        # propagate through the transformer\n",
    "        tf_input = pos + 0.1 * h.permute(0,2,1,3,4).flatten(2).permute(2,0,1)\n",
    "        if self.debug: print(f'tf_input: {tf_input.shape}')\n",
    "        h = self.transformer(tf_input,\n",
    "                             self.query_pos.unsqueeze(1)).permute(2,1,0)\n",
    "        if self.debug: print(f'tf_out: {h.shape}')\n",
    "        h = self.lin(h)\n",
    "        if self.debug: print(f'lin: {h.shape}')\n",
    "        h = h.view(1,T,-1,H,W)\n",
    "        if self.debug: print(f'before dec: {h.shape}')\n",
    "        return self.decoder(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def detr_split(model, stacked=False):\n",
    "    if not stacked:\n",
    "        return [params(model.backbone), \n",
    "                params(model.conv)+params(model.transformer)+[model.query_pos]+[model.pos]+params(model.decoder)+params(model.lin)]\n",
    "    else:\n",
    "        return [params(model.module.backbone), \n",
    "                params(model.module.conv)+params(model.module.transformer)+[model.module.query_pos]+[model.module.pos]+params(model.module.decoder)+params(model.module.lin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detr = DETR(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=detr_split(detr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone: torch.Size([1, 5, 512, 4, 4])\n",
      "h: torch.Size([1, 5, 256, 4, 4])\n",
      "pos: torch.Size([80, 1, 256])\n",
      "tf_input: torch.Size([80, 1, 256])\n",
      "tf_out: torch.Size([256, 1, 100])\n",
      "lin: torch.Size([256, 1, 80])\n",
      "before dec: torch.Size([1, 5, 256, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1, 128, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detr(torch.rand(1,5,1,128,128)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel = StackUnstack(detr)\n",
    "imgs_list = [torch.rand(1,1,128,128) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = detr_split(smodel, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone: torch.Size([1, 5, 512, 4, 4])\n",
      "h: torch.Size([1, 5, 256, 4, 4])\n",
      "pos: torch.Size([80, 1, 256])\n",
      "tf_input: torch.Size([80, 1, 256])\n",
      "tf_out: torch.Size([256, 1, 100])\n",
      "lin: torch.Size([256, 1, 80])\n",
      "before dec: torch.Size([1, 5, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "smodel(imgs_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Transformer\n",
    "> https://github.com/maxjcohen/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tst.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "d_model = 64 # Lattent dim\n",
    "q = 8 # Query size\n",
    "v = 8 # Value size\n",
    "h = 8 # Number of heads\n",
    "N = 4 # Number of encoder and decoder to stack\n",
    "attention_size = 12 # Attention window size\n",
    "dropout = 0.2 # Dropout rate\n",
    "pe = None # Positional encoding\n",
    "chunk_mode = None\n",
    "\n",
    "d_input = 256 # From dataset\n",
    "d_output = 256 # From dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = Transformer(d_input, d_model, d_output, q, v, h, N, attention_size=attention_size, dropout=dropout, chunk_mode=chunk_mode, pe=pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf(torch.rand(8,10,256)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TransformerTS(Module):\n",
    "    def __init__(self,  arch=resnet34, n_in=3, n_out=1, hidden_dim=256, debug=False):\n",
    "        self.debug = debug\n",
    "        \n",
    "        #the image encoder\n",
    "        self.backbone = TimeDistributed(Encoder(arch, n_in=n_in, pretrained=True))\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = TimeDistributed(nn.Conv2d(512, hidden_dim, 1))\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        q = 8 # Query size\n",
    "        v = 8 # Value size\n",
    "        h = 8 # Number of heads\n",
    "        n = 4 # Number of encoder and decoder to stack\n",
    "        attention_size = 12 # Attention window size\n",
    "        dropout = 0.2 # Dropout rate\n",
    "        pe = None # Positional encoding\n",
    "        chunk_mode = None\n",
    "        self.transformer = Transformer(hidden_dim, hidden_dim, hidden_dim, q, v, h, n, attention_size, dropout, chunk_mode, pe)\n",
    "\n",
    "        #decoder\n",
    "        self.decoder = TimeDistributed(nn.Sequential(\n",
    "                           UpsampleBlock(256, 128, residual=False),\n",
    "                           UpsampleBlock(128, 128, residual=False),\n",
    "                           UpsampleBlock(128, 64, residual=False),\n",
    "                           UpsampleBlock(64, 32, residual=False),\n",
    "                           UpsampleBlock(32, 16, residual=False),\n",
    "                           nn.Conv2d(16, n_out, 3,1,1))\n",
    "                                      )\n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet up to avg-pool layer\n",
    "        \n",
    "        x = self.backbone(inputs)\n",
    "        if self.debug: print(f'backbone: {x.shape}')\n",
    "            \n",
    "        # convert from the latent dim to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "        if self.debug: print(f'h: {h.shape}')\n",
    "        bs,T,_, H,W = h.shape\n",
    "        \n",
    "        tf_input = h.permute(0,2,1,3,4).flatten(2).permute(0,2,1)\n",
    "        if self.debug: print(f'tf_input: {tf_input.shape}')\n",
    "        h = self.transformer(tf_input)\n",
    "        if self.debug: print(f'tf_out: {h.shape}')\n",
    "        h = h.view(bs,T,-1,H,W)\n",
    "        if self.debug: print(f'before dec: {h.shape}')\n",
    "        return self.decoder(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tf_split(m, stacked=False):\n",
    "    if not stacked:\n",
    "        return [params(m.backbone), \n",
    "                params(m.conv)+params(m.transformer)+params(m.decoder)]\n",
    "    else:\n",
    "        return [params(m.module.backbone), \n",
    "                params(m.module.conv)+params(m.module.transformer)+params(m.module.decoder)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfts = TransformerTS(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone: torch.Size([2, 5, 512, 4, 4])\n",
      "h: torch.Size([2, 5, 256, 4, 4])\n",
      "tf_input: torch.Size([2, 80, 256])\n",
      "tf_out: torch.Size([2, 80, 256])\n",
      "before dec: torch.Size([2, 5, 256, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 1, 128, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfts(torch.rand(2,5,3,128,128)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_models.conv_rnn.ipynb.\n",
      "Converted 02_models.transformer.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
