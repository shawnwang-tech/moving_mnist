{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer model\n",
    "> inspired from DETR : https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb#scrollTo=h91rsIPl7tVl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.vision.all import *\n",
    "from moving_mnist.models.conv_rnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2070 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(1)\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(create_cnn_model)\n",
    "class Encoder(Module):\n",
    "    def __init__(self, arch=resnet34, n_in=3, weights_file=None, n_out=1, strict=False, pretrained=False, **kwargs):\n",
    "        \"Encoder based on resnet, returns the feature map\"\n",
    "        model = create_cnn_model(arch, n_out=n_out, n_in=n_in, pretrained=pretrained, **kwargs)\n",
    "        if weights_file is not None: \n",
    "            load_res = load_model(weights_file, model, opt=None, strict=strict)\n",
    "            print(f'Loading model from file {weights_file} \\n>missing keys: {load_res}')\n",
    "        self.body = model[0]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use any torchvision architecture model (resnet, vgg, inception, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r34_encoder = Encoder(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model encodes an image to a 512 feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r34_encoder(torch.rand(8, 3, 128, 128)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recover a Tensor that has `512` channels and `(4,4)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTERDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DETRdemo(nn.Module):\n",
    "    \"\"\"\n",
    "    Demo DETR implementation.\n",
    "\n",
    "    Demo implementation of DETR in minimal number of lines, with the\n",
    "    following differences wrt DETR in the paper:\n",
    "    * learned positional encoding (instead of sine)\n",
    "    * positional encoding is passed at input (instead of attention)\n",
    "    * fc bbox predictor (instead of MLP)\n",
    "    The model achieves ~40 AP on COCO val5k and runs at ~28 FPS on Tesla V100.\n",
    "    Only batch size 1 supported.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden_dim=256, nheads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, debug=False):\n",
    "        super().__init__()\n",
    "        self.debug = debug\n",
    "        # create ResNet-50 backbone\n",
    "        self.backbone = resnet50()\n",
    "        del self.backbone.fc\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "        # prediction heads, one extra class for predicting non-empty slots\n",
    "        # note that in baseline DETR linear_bbox layer is 3-layer MLP\n",
    "        self.linear_class = nn.Linear(hidden_dim, num_classes + 1)\n",
    "        self.linear_bbox = nn.Linear(hidden_dim, 4)\n",
    "\n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet-50 up to avg-pool layer\n",
    "        x = self.backbone.conv1(inputs)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        # convert from 2048 to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "\n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        pos = torch.cat([\n",
    "            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n",
    "            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n",
    "        ], dim=-1).flatten(0, 1).unsqueeze(1)\n",
    "        if self.debug: print(f'pos: {pos.shape}')\n",
    "        \n",
    "        tf_input = pos + 0.1 * h.flatten(2).permute(2, 0, 1)\n",
    "        if self.debug: print(f'tf_input: {tf_input.shape}')\n",
    "        # propagate through the transformer\n",
    "        h = self.transformer(tf_input,\n",
    "                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
    "        if self.debug: print(f'tf_out: {h.shape}')\n",
    "            \n",
    "        # finally project transformer outputs to class labels and bounding boxes\n",
    "        return {'pred_logits': self.linear_class(h), \n",
    "                'pred_boxes': self.linear_bbox(h).sigmoid()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = DETRdemo(10, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: torch.Size([16, 1, 256])\n",
      "tf_input: torch.Size([16, 1, 256])\n",
      "tf_out: torch.Size([1, 100, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pred_logits': tensor([[[ 0.3864, -0.2979,  1.3025,  ..., -0.5235, -0.2826,  0.1175],\n",
       "          [-0.4414, -0.5061,  1.1318,  ..., -0.4527, -0.2506,  0.5684],\n",
       "          [ 0.0783, -0.2800,  1.1213,  ..., -0.4288, -0.2279,  0.2643],\n",
       "          ...,\n",
       "          [-0.0739, -0.3486,  1.3377,  ..., -0.9759, -0.1284,  0.9343],\n",
       "          [ 0.2017, -0.4327,  1.2409,  ..., -0.9430, -0.5388,  0.3167],\n",
       "          [ 0.1249, -0.1855,  1.2717,  ..., -0.5588, -0.1075,  0.5636]]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " 'pred_boxes': tensor([[[0.3038, 0.5805, 0.6628, 0.3024],\n",
       "          [0.3248, 0.4435, 0.7345, 0.3337],\n",
       "          [0.3113, 0.5762, 0.7161, 0.3430],\n",
       "          [0.4082, 0.5398, 0.5523, 0.3664],\n",
       "          [0.3668, 0.4667, 0.6369, 0.4547],\n",
       "          [0.3351, 0.4824, 0.6206, 0.5372],\n",
       "          [0.3091, 0.4184, 0.6198, 0.3836],\n",
       "          [0.4208, 0.4537, 0.5953, 0.4685],\n",
       "          [0.3357, 0.4424, 0.6408, 0.4808],\n",
       "          [0.2996, 0.3632, 0.6173, 0.3693],\n",
       "          [0.2764, 0.4800, 0.7029, 0.3758],\n",
       "          [0.3458, 0.4668, 0.7318, 0.4128],\n",
       "          [0.2609, 0.3919, 0.5137, 0.4172],\n",
       "          [0.2500, 0.5241, 0.6348, 0.3410],\n",
       "          [0.2846, 0.4986, 0.5780, 0.3823],\n",
       "          [0.2770, 0.5259, 0.6209, 0.3442],\n",
       "          [0.3006, 0.5530, 0.6618, 0.4031],\n",
       "          [0.3386, 0.5059, 0.6492, 0.4741],\n",
       "          [0.3123, 0.5545, 0.6533, 0.5209],\n",
       "          [0.3123, 0.4557, 0.5973, 0.3541],\n",
       "          [0.3332, 0.3905, 0.6070, 0.4866],\n",
       "          [0.3309, 0.3637, 0.6748, 0.4445],\n",
       "          [0.2699, 0.4640, 0.6747, 0.2776],\n",
       "          [0.2812, 0.4390, 0.6393, 0.3809],\n",
       "          [0.3580, 0.4149, 0.6355, 0.3698],\n",
       "          [0.3466, 0.4499, 0.5396, 0.3077],\n",
       "          [0.3916, 0.4030, 0.6684, 0.4184],\n",
       "          [0.1960, 0.5095, 0.6071, 0.4757],\n",
       "          [0.3575, 0.4487, 0.6357, 0.3676],\n",
       "          [0.2912, 0.4293, 0.6866, 0.4244],\n",
       "          [0.4898, 0.5286, 0.7331, 0.3846],\n",
       "          [0.3467, 0.4544, 0.5889, 0.4718],\n",
       "          [0.3330, 0.4916, 0.6463, 0.3873],\n",
       "          [0.3165, 0.5006, 0.6530, 0.4418],\n",
       "          [0.2839, 0.4779, 0.6528, 0.3769],\n",
       "          [0.2866, 0.4657, 0.6343, 0.4416],\n",
       "          [0.3006, 0.5127, 0.6861, 0.4101],\n",
       "          [0.2863, 0.4728, 0.5867, 0.4234],\n",
       "          [0.3083, 0.4656, 0.5655, 0.3912],\n",
       "          [0.2389, 0.5419, 0.6310, 0.3466],\n",
       "          [0.3360, 0.4772, 0.6159, 0.4273],\n",
       "          [0.3196, 0.5195, 0.6023, 0.4446],\n",
       "          [0.3586, 0.5273, 0.6641, 0.3790],\n",
       "          [0.3731, 0.4615, 0.6017, 0.4023],\n",
       "          [0.4450, 0.4967, 0.5792, 0.4956],\n",
       "          [0.2903, 0.3654, 0.6733, 0.4380],\n",
       "          [0.2480, 0.5727, 0.5586, 0.3998],\n",
       "          [0.3571, 0.4290, 0.6931, 0.4295],\n",
       "          [0.3834, 0.4950, 0.4796, 0.4204],\n",
       "          [0.3913, 0.5648, 0.6601, 0.3467],\n",
       "          [0.3508, 0.5479, 0.6483, 0.4064],\n",
       "          [0.2920, 0.5177, 0.6112, 0.4402],\n",
       "          [0.4250, 0.4670, 0.6288, 0.3328],\n",
       "          [0.3513, 0.4411, 0.6190, 0.3660],\n",
       "          [0.3451, 0.5522, 0.5350, 0.4158],\n",
       "          [0.3702, 0.3923, 0.5839, 0.4086],\n",
       "          [0.3197, 0.4084, 0.6243, 0.3272],\n",
       "          [0.3757, 0.5275, 0.6208, 0.4743],\n",
       "          [0.3917, 0.4485, 0.6737, 0.3854],\n",
       "          [0.2796, 0.4122, 0.6562, 0.3471],\n",
       "          [0.3190, 0.3583, 0.6120, 0.2809],\n",
       "          [0.4143, 0.4578, 0.5595, 0.4887],\n",
       "          [0.3603, 0.5060, 0.6127, 0.3582],\n",
       "          [0.3560, 0.5308, 0.6069, 0.3854],\n",
       "          [0.3491, 0.5382, 0.5478, 0.5092],\n",
       "          [0.3369, 0.4772, 0.6293, 0.5015],\n",
       "          [0.3856, 0.4886, 0.6079, 0.3763],\n",
       "          [0.3208, 0.5190, 0.5366, 0.3299],\n",
       "          [0.3173, 0.4724, 0.5758, 0.3280],\n",
       "          [0.2609, 0.4934, 0.6336, 0.4774],\n",
       "          [0.2684, 0.4484, 0.6825, 0.4357],\n",
       "          [0.2843, 0.4393, 0.6172, 0.3812],\n",
       "          [0.3381, 0.4616, 0.6400, 0.4278],\n",
       "          [0.2607, 0.5313, 0.6807, 0.4063],\n",
       "          [0.3417, 0.4485, 0.5088, 0.4546],\n",
       "          [0.3238, 0.4868, 0.5800, 0.4341],\n",
       "          [0.2753, 0.4183, 0.6427, 0.3875],\n",
       "          [0.3032, 0.5341, 0.6339, 0.3633],\n",
       "          [0.3531, 0.4781, 0.6868, 0.3442],\n",
       "          [0.3157, 0.4325, 0.5747, 0.4408],\n",
       "          [0.2943, 0.5238, 0.4821, 0.5557],\n",
       "          [0.2844, 0.4757, 0.6273, 0.4489],\n",
       "          [0.2685, 0.4324, 0.5710, 0.4178],\n",
       "          [0.3451, 0.4622, 0.6541, 0.4155],\n",
       "          [0.3432, 0.4287, 0.6559, 0.4199],\n",
       "          [0.4192, 0.5428, 0.5595, 0.4666],\n",
       "          [0.3676, 0.5320, 0.5612, 0.4427],\n",
       "          [0.2801, 0.4637, 0.7073, 0.3818],\n",
       "          [0.2890, 0.4660, 0.6463, 0.3902],\n",
       "          [0.3873, 0.4014, 0.6583, 0.3190],\n",
       "          [0.3309, 0.5640, 0.5417, 0.3598],\n",
       "          [0.4633, 0.5495, 0.6263, 0.4932],\n",
       "          [0.2742, 0.5211, 0.5765, 0.3822],\n",
       "          [0.3894, 0.4888, 0.5615, 0.4447],\n",
       "          [0.3049, 0.4655, 0.6197, 0.4312],\n",
       "          [0.4482, 0.5522, 0.6700, 0.4743],\n",
       "          [0.2713, 0.5345, 0.5796, 0.4370],\n",
       "          [0.3789, 0.4008, 0.6648, 0.4488],\n",
       "          [0.2870, 0.4582, 0.6221, 0.3944],\n",
       "          [0.2966, 0.5449, 0.7050, 0.4477]]], grad_fn=<SigmoidBackward>)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo(torch.rand(1,3,128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try an architecture with an Encoder/Decoder model provided by the Transformer, instead of the ConvGRU layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DETR(Module):\n",
    "    def __init__(self,  n_in=1, n_out=1, hidden_dim=256, nheads=8, num_encoder_layers=6, \n",
    "                 num_decoder_layers=6, debug=False):\n",
    "        self.debug = debug\n",
    "        \n",
    "        #the image encoder\n",
    "        self.backbone = TimeDistributed(Encoder(n_in=n_in))\n",
    "\n",
    "        # create conversion layer\n",
    "        self.conv = TimeDistributed(nn.Conv2d(512, hidden_dim, 1))\n",
    "\n",
    "        # create a default PyTorch transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n",
    "        \n",
    "        # output positional encodings (object queries)\n",
    "        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))\n",
    "\n",
    "        # spatial positional encodings\n",
    "        # note that in baseline DETR we use sine positional encodings\n",
    "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 4))\n",
    "        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 4))\n",
    "        self.time_embed = nn. Parameter(torch.rand(50, hidden_dim //2))\n",
    "        \n",
    "        #decoder\n",
    "        self.decoder = TimeDistributed(nn.Sequential(\n",
    "                      UpsampleBlock(256, 128, residual=False),\n",
    "                      UpsampleBlock(128, 128, residual=False),\n",
    "                      UpsampleBlock(128, 64, residual=False),\n",
    "                      UpsampleBlock(64, 32, residual=False),\n",
    "                      UpsampleBlock(32, 16, residual=False),\n",
    "                      nn.Conv2d(16, n_out, 1)\n",
    "                    ))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # propagate inputs through ResNet up to avg-pool layer\n",
    "        x = self.backbone(inputs)\n",
    "        if self.debug: print(f'backbone: {x.shape}')\n",
    "            \n",
    "        # convert from the latent dim to 256 feature planes for the transformer\n",
    "        h = self.conv(x)\n",
    "        if self.debug: print(f'h: {h.shape}')\n",
    "            \n",
    "        # construct positional encodings\n",
    "        H, W = h.shape[-2:]\n",
    "        T = h.shape[1]\n",
    "        pos = torch.cat([\n",
    "            self.time_embed[:T].view(T,1,1,-1).repeat(1, H, W, 1),\n",
    "            self.col_embed[:W].view(1,1,W,-1).repeat(T, H, 1, 1),\n",
    "            self.row_embed[:H].view(1,H,1,-1).repeat(T, 1, W, 1),\n",
    "        ], dim=-1).flatten(0, 2).unsqueeze(1)\n",
    "        if self.debug: print(f'pos: {pos.shape}')\n",
    "        \n",
    "        # propagate through the transformer\n",
    "        tf_input = pos + 0.1 * h.permute(0,2,1,3,4).flatten(2).permute(2,0,1)\n",
    "        if self.debug: print(f'tf_input: {tf_input.shape}')\n",
    "        h = self.transformer(tf_input,\n",
    "                             self.query_pos.unsqueeze(1)).transpose(0, 1)\n",
    "        if self.debug: print(f'tf_out: {h.shape}')\n",
    "        return self.decoder(h[:,0:H*W*T, :].view(1,T,-1,H,W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detr = DETR(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = torch.rand(1, 5, 256, 4, 4)\n",
    "\n",
    "# h.permute(0,2,1,3,4).flatten(2).permute(2,0,1).shape\n",
    "\n",
    "# H, W = h.shape[-2:]\n",
    "# T = h.shape[1]\n",
    "\n",
    "# h.flatten(3).shape\n",
    "\n",
    "# T\n",
    "\n",
    "# detr.time_embed[:T].view(T,1,1,-1).repeat(1, H, W, 1).shape\n",
    "\n",
    "# detr.col_embed[:W].view(1,1,W,-1).repeat(T, H, 1, 1).shape\n",
    "\n",
    "# detr.row_embed[:H].view(1,H,1,-1).repeat(T, 1, W, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone: torch.Size([1, 5, 512, 4, 4])\n",
      "h: torch.Size([1, 5, 256, 4, 4])\n",
      "pos: torch.Size([80, 1, 256])\n",
      "tf_input: torch.Size([80, 1, 256])\n",
      "tf_out: torch.Size([1, 100, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1, 128, 128])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detr(torch.rand(1,5,1,128,128)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_models.conv_rnn.ipynb.\n",
      "Converted 02_models.transformer.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
