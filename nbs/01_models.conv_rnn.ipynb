{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.conv_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrecnt Convolutional Kernels\n",
    "> ConvLSTM and ConvGRU cells and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai2.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2070 SUPER'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(1)\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvGRU_cell(Module):\n",
    "    def __init__(self, in_ch, out_ch, ks=3, debug=False):\n",
    "        self.in_ch = in_ch\n",
    "        # kernel_size of input_to_state equals state_to_state\n",
    "        self.ks = ks\n",
    "        self.out_ch = out_ch\n",
    "        self.debug = debug\n",
    "        self.padding = (ks - 1) // 2\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(self.in_ch + self.out_ch,\n",
    "                      2 * self.out_ch, self.ks, 1,\n",
    "                      self.padding),\n",
    "            nn.GroupNorm(2 * self.out_ch // 32, 2 * self.out_ch))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(self.in_ch + self.out_ch,\n",
    "                      self.out_ch, self.ks, 1, self.padding),\n",
    "            nn.GroupNorm(self.out_ch // 32, self.out_ch))\n",
    "\n",
    "    def forward(self, inputs, hidden_state=None):\n",
    "        \"inputs shape: (bs, seq_len, ch, w, h)\"\n",
    "        bs, seq_len, ch, w, h = inputs.shape\n",
    "        if hidden_state is None:\n",
    "            htprev = self.initHidden(bs, self.out_ch, w, h)\n",
    "            if self.debug: print(f'htprev: {htprev.shape}')\n",
    "        else:\n",
    "            htprev = hidden_state\n",
    "        output_inner = []\n",
    "        for index in range(seq_len):\n",
    "            x = inputs[:, index, ...]\n",
    "            combined_1 = torch.cat((x, htprev), 1)  # X_t + H_t-1\n",
    "            gates = self.conv1(combined_1)  # W * (X_t + H_t-1)          \n",
    "            zgate, rgate = torch.split(gates, self.out_ch, dim=1)\n",
    "            z = torch.sigmoid(zgate)\n",
    "            r = torch.sigmoid(rgate)\n",
    "            combined_2 = torch.cat((x, r * htprev),1)\n",
    "            ht = self.conv2(combined_2)\n",
    "            ht = torch.tanh(ht)\n",
    "            htnext = (1 - z) * htprev + z * ht\n",
    "            output_inner.append(htnext)\n",
    "            htprev = htnext\n",
    "        return torch.stack(output_inner, dim=1), htnext\n",
    "    def __repr__(self): return f'ConvGRU_cell(in={self.in_ch}, out={self.out_ch}, ks={self.ks})'\n",
    "    def initHidden(self, bs, ch, w, h): return one_param(self).new_zeros(bs, ch, w, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvGRU_cell(in=32, out=32, ks=3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell = ConvGRU_cell(32, 32, debug=True).cuda()\n",
    "cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "htprev: torch.Size([2, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 7, 32, 64, 64).cuda()\n",
    "out, h = cell(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 32, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(out.shape, x.shape) \n",
    "test_eq(h.shape, [2,32,64,64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be possible to call with hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2, h2 = cell(out, h)\n",
    "test_eq(h2.shape, [2, 32, 64, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very nasty module to propagate 2D layers over sequence of images, inspired from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TimeDistributed(Module):\n",
    "    \"Applies a module over tdim identically for each step\" \n",
    "    def __init__(self, module, low_mem=False, tdim=1):\n",
    "        self.module = module\n",
    "        self.low_mem = low_mem\n",
    "        self.tdim = tdim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"input x with shape:(bs,steps,channels,width,height)\"\n",
    "        if self.low_mem or self.tdim!=1: \n",
    "            return self.low_mem_forward(x)\n",
    "        else:\n",
    "            inp_shape = x.shape\n",
    "            bs, seq_len = inp_shape[0], inp_shape[1]   \n",
    "            out = self.module(x.view(bs*seq_len, *inp_shape[2:]))\n",
    "            out_shape = out.shape\n",
    "            return out.view(bs, seq_len,*out_shape[1:])\n",
    "        \n",
    "    def low_mem_forward(self, x):                                           \n",
    "        \"input x with shape:(bs,steps,channels,width,height)\"\n",
    "        x_split = torch.split(x,1,dim=self.tdim)\n",
    "        out =[]\n",
    "        for i in range(len(x_split)):\n",
    "            out.append(self.module(x_split[i].squeeze(dim=self.tdim)))\n",
    "        return torch.stack(out,dim=self.tdim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Encoder(Module):\n",
    "    def __init__(self, n_in=1, szs=[16,64,96,96], ks=3, rnn_ks=5, act=nn.ReLU, norm=None, debug=False):\n",
    "        self.n_blocks = len(szs)-1\n",
    "        self.debug = debug\n",
    "        convs = []\n",
    "        rnns = []\n",
    "        convs.append(ConvLayer(1, szs[0], ks=ks, padding=ks//2, act_cls=act, norm_type=norm))\n",
    "        rnns.append(ConvGRU_cell(szs[0], szs[1], ks=rnn_ks))\n",
    "        for ni, nf in zip(szs[1:-1], szs[2:]):\n",
    "            if self.debug: print(ni, nf)\n",
    "            convs.append(ConvLayer(ni, ni, ks=ks, stride=2, padding=ks//2, act_cls=act, norm_type=norm))\n",
    "            rnns.append(ConvGRU_cell(ni, nf, ks=rnn_ks))\n",
    "        self.convs = nn.ModuleList(TimeDistributed(conv) for conv in convs)\n",
    "        self.rnns = nn.ModuleList(rnns)\n",
    "        \n",
    "    def forward_by_stage(self, inputs, conv, rnn):\n",
    "        if self.debug: \n",
    "            print(f' Layer: {rnn}')\n",
    "            print(' inputs: ', inputs.shape)\n",
    "        inputs = conv(inputs)\n",
    "        if self.debug: print(' after_convs: ', inputs.shape)\n",
    "        outputs_stage, state_stage = rnn(inputs, None)\n",
    "        if self.debug: print(' output_stage: ', outputs_stage.shape)\n",
    "        return outputs_stage, state_stage\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"inputs.shape bs,seq_len,1,64,64\"\n",
    "        hidden_states = []\n",
    "        for i, (conv, rnn) in enumerate(zip(self.convs, self.rnns)):\n",
    "            if self.debug: print('stage: ',i)\n",
    "            inputs, state_stage = self.forward_by_stage(inputs, conv, rnn)\n",
    "            hidden_states.append(state_stage)\n",
    "        return inputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 96\n",
      "96 96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (convs): ModuleList(\n",
       "    (0): TimeDistributed(\n",
       "      (module): ConvLayer(\n",
       "        (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): TimeDistributed(\n",
       "      (module): ConvLayer(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): TimeDistributed(\n",
       "      (module): ConvLayer(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rnns): ModuleList(\n",
       "    (0): ConvGRU_cell(in=16, out=64, ks=5)\n",
       "    (1): ConvGRU_cell(in=64, out=96, ks=5)\n",
       "    (2): ConvGRU_cell(in=96, out=96, ks=5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(debug=True)\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.rand(2, 10, 1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage:  0\n",
      " Layer: ConvGRU_cell(in=16, out=64, ks=5)\n",
      " inputs:  torch.Size([2, 10, 1, 64, 64])\n",
      " after_convs:  torch.Size([2, 10, 16, 64, 64])\n",
      " output_stage:  torch.Size([2, 10, 64, 64, 64])\n",
      "stage:  1\n",
      " Layer: ConvGRU_cell(in=64, out=96, ks=5)\n",
      " inputs:  torch.Size([2, 10, 64, 64, 64])\n",
      " after_convs:  torch.Size([2, 10, 64, 32, 32])\n",
      " output_stage:  torch.Size([2, 10, 96, 32, 32])\n",
      "stage:  2\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:  torch.Size([2, 10, 96, 32, 32])\n",
      " after_convs:  torch.Size([2, 10, 96, 16, 16])\n",
      " output_stage:  torch.Size([2, 10, 96, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "enc_out, h = enc(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 96, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class UpsampleBlock(Module):\n",
    "    \"A quasi-UNet block, using `PixelShuffle_ICNR upsampling`.\"\n",
    "    @delegates(ConvLayer.__init__)\n",
    "    def __init__(self, in_ch, out_ch, final_div=True, blur=False, act_cls=defaults.activation,\n",
    "                 self_attention=False, init=nn.init.kaiming_normal_, norm_type=None, **kwargs):\n",
    "        store_attr(self, 'in_ch,out_ch,blur,act_cls,self_attention,norm_type')\n",
    "        self.shuf = PixelShuffle_ICNR(in_ch, in_ch//2, blur=blur, act_cls=act_cls, norm_type=norm_type)\n",
    "        ni = in_ch//2\n",
    "        nf = out_ch\n",
    "        self.conv1 = ConvLayer(ni, nf, act_cls=act_cls, norm_type=norm_type, **kwargs)\n",
    "        self.conv2 = ConvLayer(nf, nf, act_cls=act_cls, norm_type=norm_type,\n",
    "                               xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n",
    "        self.relu = act_cls()\n",
    "        apply_init(nn.Sequential(self.conv1, self.conv2), init)\n",
    "    def __repr__(self): return (f'UpsampleBLock(in={self.in_ch}, out={self.out_ch}, blur={self.blur}, '\n",
    "                                f'act={self.act_cls()}, attn={self.self_attention}, norm={self.norm_type})')\n",
    "    def forward(self, up_in):\n",
    "        up_out = self.shuf(up_in)\n",
    "        return self.conv2(self.conv1(up_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpsampleBLock(in=32, out=16, blur=False, act=ReLU(), attn=False, norm=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us = UpsampleBlock(32, 16)\n",
    "us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us(torch.rand(8, 32, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Decoder(Module):\n",
    "    def __init__(self, n_out=1, szs=[16,64,96,96], ks=3, rnn_ks=5, act=nn.ReLU, blur=False, attn=False, \n",
    "                 norm=None, debug=False):\n",
    "        self.n_blocks = len(szs)-1\n",
    "        self.debug = debug\n",
    "        deconvs = []\n",
    "        rnns = []\n",
    "        szs = szs[::-1]\n",
    "        rnns.append(ConvGRU_cell(szs[0], szs[0], ks=rnn_ks))\n",
    "        for ni, nf in zip(szs[0:-2], szs[1:]):\n",
    "            deconvs.append(UpsampleBlock(ni, ni, blur=blur, self_attention=attn, act_cls=act, norm_type=norm))\n",
    "            rnns.append(ConvGRU_cell(ni, nf, ks=rnn_ks))\n",
    "        \n",
    "        #last layer\n",
    "        deconvs.append(ConvLayer(szs[-2], szs[-1], ks, padding=ks//2, act_cls=act, norm_type=norm))\n",
    "        self.head = TimeDistributed(nn.Conv2d(szs[-1], n_out,kernel_size=1))\n",
    "        self.deconvs = nn.ModuleList(TimeDistributed(conv) for conv in deconvs)\n",
    "        self.rnns = nn.ModuleList(rnns)\n",
    "\n",
    "    def forward_by_stage(self, inputs, state, deconv, rnn):\n",
    "        if self.debug: \n",
    "            print(f' Layer: {rnn}')\n",
    "            print(' inputs:, state: ', inputs.shape, state.shape)\n",
    "        inputs, state_stage = rnn(inputs, state)\n",
    "        if self.debug: \n",
    "            print(' after rnn: ', inputs.shape)\n",
    "            print(f' Layer: {deconv}')\n",
    "        outputs_stage = deconv(inputs)\n",
    "        if self.debug: print(' after_deconvs: ', outputs_stage.shape)\n",
    "        return outputs_stage, state_stage\n",
    "    \n",
    "    def forward(self, inputs, hidden_states):\n",
    "        for i, (state, conv, rnn) in enumerate(zip(hidden_states[::-1], self.deconvs, self.rnns)):\n",
    "            if self.debug: print('stage: ',i)\n",
    "            inputs, state_stage = self.forward_by_stage(inputs, state, conv, rnn)\n",
    "        return self.head(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (head): TimeDistributed(\n",
       "    (module): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (deconvs): ModuleList(\n",
       "    (0): TimeDistributed(\n",
       "      (module): UpsampleBLock(in=96, out=96, blur=False, act=ReLU(), attn=False, norm=None)\n",
       "    )\n",
       "    (1): TimeDistributed(\n",
       "      (module): UpsampleBLock(in=96, out=96, blur=False, act=ReLU(), attn=False, norm=None)\n",
       "    )\n",
       "    (2): TimeDistributed(\n",
       "      (module): ConvLayer(\n",
       "        (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (rnns): ModuleList(\n",
       "    (0): ConvGRU_cell(in=96, out=96, ks=5)\n",
       "    (1): ConvGRU_cell(in=96, out=96, ks=5)\n",
       "    (2): ConvGRU_cell(in=96, out=64, ks=5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = Decoder(debug=True)\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 64, 64, 64]),\n",
       " torch.Size([2, 96, 32, 32]),\n",
       " torch.Size([2, 96, 16, 16])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.shape for _ in h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 96, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage:  0\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 96, 16, 16]) torch.Size([2, 96, 16, 16])\n",
      " after rnn:  torch.Size([2, 10, 96, 16, 16])\n",
      " Layer: TimeDistributed(\n",
      "  (module): UpsampleBLock(in=96, out=96, blur=False, act=ReLU(), attn=False, norm=None)\n",
      ")\n",
      " after_deconvs:  torch.Size([2, 10, 96, 32, 32])\n",
      "stage:  1\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 96, 32, 32]) torch.Size([2, 96, 32, 32])\n",
      " after rnn:  torch.Size([2, 10, 96, 32, 32])\n",
      " Layer: TimeDistributed(\n",
      "  (module): UpsampleBLock(in=96, out=96, blur=False, act=ReLU(), attn=False, norm=None)\n",
      ")\n",
      " after_deconvs:  torch.Size([2, 10, 96, 64, 64])\n",
      "stage:  2\n",
      " Layer: ConvGRU_cell(in=96, out=64, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 96, 64, 64]) torch.Size([2, 64, 64, 64])\n",
      " after rnn:  torch.Size([2, 10, 64, 64, 64])\n",
      " Layer: TimeDistributed(\n",
      "  (module): ConvLayer(\n",
      "    (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      " after_deconvs:  torch.Size([2, 10, 16, 64, 64])\n",
      "---\n",
      "\n",
      "stage:  0\n",
      " Layer: ConvGRU_cell(in=16, out=64, ks=5)\n",
      " inputs:  torch.Size([2, 10, 1, 64, 64])\n",
      " after_convs:  torch.Size([2, 10, 16, 64, 64])\n",
      " output_stage:  torch.Size([2, 10, 64, 64, 64])\n",
      "stage:  1\n",
      " Layer: ConvGRU_cell(in=64, out=96, ks=5)\n",
      " inputs:  torch.Size([2, 10, 64, 64, 64])\n",
      " after_convs:  torch.Size([2, 10, 64, 32, 32])\n",
      " output_stage:  torch.Size([2, 10, 96, 32, 32])\n",
      "stage:  2\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:  torch.Size([2, 10, 96, 32, 32])\n",
      " after_convs:  torch.Size([2, 10, 96, 16, 16])\n",
      " output_stage:  torch.Size([2, 10, 96, 16, 16])\n",
      "stage:  0\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 96, 16, 16]) torch.Size([2, 96, 16, 16])\n",
      " after rnn:  torch.Size([2, 10, 96, 16, 16])\n",
      " Layer: TimeDistributed(\n",
      "  (module): UpsampleBLock(in=96, out=96, blur=False, act=ReLU(), attn=False, norm=None)\n",
      ")\n",
      " after_deconvs:  torch.Size([2, 10, 96, 32, 32])\n",
      "stage:  1\n",
      " Layer: ConvGRU_cell(in=96, out=96, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 96, 32, 32]) torch.Size([2, 96, 32, 32])\n",
      " after rnn:  torch.Size([2, 10, 96, 32, 32])\n",
      " Layer: TimeDistributed(\n",
      "  (module): UpsampleBLock(in=96, out=96, blur=False, act=ReLU(), attn=False, norm=None)\n",
      ")\n",
      " after_deconvs:  torch.Size([2, 10, 96, 64, 64])\n",
      "stage:  2\n",
      " Layer: ConvGRU_cell(in=96, out=64, ks=5)\n",
      " inputs:, state:  torch.Size([2, 10, 96, 64, 64]) torch.Size([2, 64, 64, 64])\n",
      " after rnn:  torch.Size([2, 10, 64, 64, 64])\n",
      " Layer: TimeDistributed(\n",
      "  (module): ConvLayer(\n",
      "    (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      " after_deconvs:  torch.Size([2, 10, 16, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test_eq(dec(enc_out, h).shape, imgs.shape)\n",
    "print('---\\n')\n",
    "test_eq(dec(*enc(imgs)).shape, imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SimpleModel(Module):\n",
    "    def __init__(self, n_in=1, n_out=1, szs=[16,64,96,96], ks=3, rnn_ks=5, act=nn.ReLU, blur=False, attn=False, \n",
    "                 norm=None, strategy='zero', debug=False):\n",
    "        self.strategy = strategy\n",
    "        self.encoder = Encoder(n_in, szs, ks, rnn_ks, act, norm, debug)\n",
    "        self.decoder = Decoder(n_out, szs, ks, rnn_ks, act, blur, attn, norm, debug)\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list) or isinstance(x, tuple):\n",
    "            x = torch.stack(x, dim=1)\n",
    "        enc_out, h = self.encoder(x)\n",
    "        if self.strategy is 'zero':\n",
    "            dec_in = one_param(self).new_zeros(*enc_out.shape)\n",
    "        elif self.strategy is 'encoder':\n",
    "            dec_in = enc_out.detach()\n",
    "        return self.decoder(dec_in, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SimpleModel(strategy='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_list = [torch.rand(2,1,64,64) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(m(imgs_list).shape, imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(m(imgs).shape, imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 01_models.conv_rnn.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
