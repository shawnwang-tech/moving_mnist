# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_models.transformer.ipynb (unless otherwise specified).

__all__ = ['Encoder', 'DETR']

# Cell
from fastai2.vision.all import *
from .conv_rnn import *

# Cell
@delegates(create_cnn_model)
class Encoder(Module):
    def __init__(self, arch=resnet34, n_in=3, weights_file=None, n_out=1, strict=False, pretrained=False, **kwargs):
        "Encoder based on resnet, returns the feature map"
        model = create_cnn_model(arch, n_out=n_out, n_in=n_in, pretrained=pretrained, **kwargs)
        if weights_file is not None:
            load_res = load_model(weights_file, model, opt=None, strict=strict)
            print(f'Loading model from file {weights_file} \n>missing keys: {load_res}')
        self.body = model[0]

    def forward(self, x):
        return self.body(x)

# Cell
class DETR(Module):
    def __init__(self,  n_in=1, n_out=1, hidden_dim=256, nheads=8, num_encoder_layers=6,
                 num_decoder_layers=6, debug=False):
        self.debug = debug

        #the image encoder
        self.backbone = TimeDistributed(Encoder(n_in=n_in))

        # create conversion layer
        self.conv = TimeDistributed(nn.Conv2d(512, hidden_dim, 1))

        # create a default PyTorch transformer
        self.transformer = nn.Transformer(
            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)

        # output positional encodings (object queries)
        self.query_pos = nn.Parameter(torch.rand(100, hidden_dim))

        # spatial positional encodings
        # note that in baseline DETR we use sine positional encodings
        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 4))
        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 4))
        self.time_embed = nn. Parameter(torch.rand(50, hidden_dim //2))

        #decoder
        self.decoder = TimeDistributed(nn.Sequential(
                      UpsampleBlock(256, 128, residual=False),
                      UpsampleBlock(128, 128, residual=False),
                      UpsampleBlock(128, 64, residual=False),
                      UpsampleBlock(64, 32, residual=False),
                      UpsampleBlock(32, 16, residual=False),
                      nn.Conv2d(16, n_out, 1)
                    ))

    def forward(self, inputs):
        # propagate inputs through ResNet up to avg-pool layer
        x = self.backbone(inputs)
        if self.debug: print(f'backbone: {x.shape}')

        # convert from the latent dim to 256 feature planes for the transformer
        h = self.conv(x)
        if self.debug: print(f'h: {h.shape}')

        # construct positional encodings
        H, W = h.shape[-2:]
        T = h.shape[1]
        pos = torch.cat([
            self.time_embed[:T].view(T,1,1,-1).repeat(1, H, W, 1),
            self.col_embed[:W].view(1,1,W,-1).repeat(T, H, 1, 1),
            self.row_embed[:H].view(1,H,1,-1).repeat(T, 1, W, 1),
        ], dim=-1).flatten(0, 2).unsqueeze(1)
        if self.debug: print(f'pos: {pos.shape}')

        # propagate through the transformer
        tf_input = pos + 0.1 * h.permute(0,2,1,3,4).flatten(2).permute(2,0,1)
        if self.debug: print(f'tf_input: {tf_input.shape}')
        h = self.transformer(tf_input,
                             self.query_pos.unsqueeze(1)).transpose(0, 1)
        if self.debug: print(f'tf_out: {h.shape}')
        return self.decoder(h[:,0:H*W*T, :].view(1,T,-1,H,W))