# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_models.conv_rnn.ipynb (unless otherwise specified).

__all__ = ['ConvGRU_cell', 'TimeDistributed', 'Encoder', 'UpsampleBlock', 'Decoder', 'StackUnstack', 'SimpleModel',
           'StackLoss']

# Cell
from fastai2.vision.all import *

# Cell
class ConvGRU_cell(Module):
    def __init__(self, in_ch, out_ch, ks=3, debug=False):
        self.in_ch = in_ch
        # kernel_size of input_to_state equals state_to_state
        self.ks = ks
        self.out_ch = out_ch
        self.debug = debug
        self.padding = (ks - 1) // 2
        self.conv1 = nn.Sequential(
            nn.Conv2d(self.in_ch + self.out_ch,
                      2 * self.out_ch, self.ks, 1,
                      self.padding),
            nn.GroupNorm(2 * self.out_ch // 32, 2 * self.out_ch))
        self.conv2 = nn.Sequential(
            nn.Conv2d(self.in_ch + self.out_ch,
                      self.out_ch, self.ks, 1, self.padding),
            nn.GroupNorm(self.out_ch // 32, self.out_ch))

    def forward(self, inputs, hidden_state=None):
        "inputs shape: (bs, seq_len, ch, w, h)"
        bs, seq_len, ch, w, h = inputs.shape
        if hidden_state is None:
            htprev = self.initHidden(bs, self.out_ch, w, h)
            if self.debug: print(f'htprev: {htprev.shape}')
        else:
            htprev = hidden_state
        output_inner = []
        for index in range(seq_len):
            x = inputs[:, index, ...]
            combined_1 = torch.cat((x, htprev), 1)  # X_t + H_t-1
            gates = self.conv1(combined_1)  # W * (X_t + H_t-1)
            zgate, rgate = torch.split(gates, self.out_ch, dim=1)
            z = torch.sigmoid(zgate)
            r = torch.sigmoid(rgate)
            combined_2 = torch.cat((x, r * htprev),1)
            ht = self.conv2(combined_2)
            ht = torch.tanh(ht)
            htnext = (1 - z) * htprev + z * ht
            output_inner.append(htnext)
            htprev = htnext
        return torch.stack(output_inner, dim=1), htnext
    def __repr__(self): return f'ConvGRU_cell(in={self.in_ch}, out={self.out_ch}, ks={self.ks})'
    def initHidden(self, bs, ch, w, h): return one_param(self).new_zeros(bs, ch, w, h)

# Cell
class TimeDistributed(Module):
    "Applies a module over tdim identically for each step"
    def __init__(self, module, low_mem=False, tdim=1):
        self.module = module
        self.low_mem = low_mem
        self.tdim = tdim

    def forward(self, x):
        "input x with shape:(bs,steps,channels,width,height)"
        if self.low_mem or self.tdim!=1:
            return self.low_mem_forward(x)
        else:
            inp_shape = x.shape
            bs, seq_len = inp_shape[0], inp_shape[1]
            out = self.module(x.view(bs*seq_len, *inp_shape[2:]))
            out_shape = out.shape
            return out.view(bs, seq_len,*out_shape[1:])

    def low_mem_forward(self, x):
        "input x with shape:(bs,steps,channels,width,height)"
        tlen = x.shape[self.tdim]
        x_split = torch.unbind(x, dim=self.tdim)
        out = []
        for i in range(tlen):
            out.append(self.module(x_split[i]))
        return torch.stack(out,dim=self.tdim)

# Cell
class Encoder(Module):
    def __init__(self, n_in=1, szs=[16,64,96,96], ks=3, rnn_ks=5, act=nn.ReLU, norm=None, debug=False):
        self.n_blocks = len(szs)-1
        self.debug = debug
        convs = []
        rnns = []
        convs.append(ConvLayer(1, szs[0], ks=ks, padding=ks//2, act_cls=act, norm_type=norm))
        rnns.append(ConvGRU_cell(szs[0], szs[1], ks=rnn_ks))
        for ni, nf in zip(szs[1:-1], szs[2:]):
            if self.debug: print(ni, nf)
            convs.append(ConvLayer(ni, ni, ks=ks, stride=2, padding=ks//2, act_cls=act, norm_type=norm))
            rnns.append(ConvGRU_cell(ni, nf, ks=rnn_ks))
        self.convs = nn.ModuleList(TimeDistributed(conv) for conv in convs)
        self.rnns = nn.ModuleList(rnns)

    def forward_by_stage(self, inputs, conv, rnn):
        if self.debug:
            print(f' Layer: {rnn}')
            print(' inputs: ', inputs.shape)
        inputs = conv(inputs)
        if self.debug: print(' after_convs: ', inputs.shape)
        outputs_stage, state_stage = rnn(inputs, None)
        if self.debug: print(' output_stage: ', outputs_stage.shape)
        return outputs_stage, state_stage

    def forward(self, inputs):
        "inputs.shape bs,seq_len,1,64,64"
        hidden_states = []
        outputs = []
        for i, (conv, rnn) in enumerate(zip(self.convs, self.rnns)):
            if self.debug: print('stage: ',i)
            inputs, state_stage = self.forward_by_stage(inputs, conv, rnn)
            outputs.append(inputs)
            hidden_states.append(state_stage)
        return outputs[-1], hidden_states

# Cell
class UpsampleBlock(Module):
    "A quasi-UNet block, using `PixelShuffle_ICNR upsampling`."
    @delegates(ConvLayer.__init__)
    def __init__(self, in_ch, out_ch, blur=False, act_cls=defaults.activation,
                 self_attention=False, init=nn.init.kaiming_normal_, norm_type=None, debug=False, **kwargs):
        store_attr(self, 'in_ch,out_ch,blur,act_cls,self_attention,norm_type,debug')
        self.shuf = PixelShuffle_ICNR(in_ch, in_ch//2, blur=blur, act_cls=act_cls, norm_type=norm_type)
        ni = in_ch//2
        nf = out_ch
        self.conv1 = ConvLayer(ni, nf, act_cls=act_cls, norm_type=norm_type, **kwargs)
        self.conv2 = ConvLayer(nf, nf, act_cls=act_cls, norm_type=norm_type,
                               xtra=SelfAttention(nf) if self_attention else None, **kwargs)
        self.relu = act_cls()
        apply_init(nn.Sequential(self.conv1, self.conv2), init)
    def __repr__(self): return (f'UpsampleBLock(in={self.in_ch}, out={self.out_ch}, blur={self.blur}, '
                                f'act={self.act_cls()}, attn={self.self_attention}, norm={self.norm_type})')
    def forward(self, up_in):
        up_out = self.shuf(up_in)
        if self.debug: print(f'up_out: {up_out.shape}')
        return self.conv2(self.conv1(up_out))

# Cell
class Decoder(Module):
    def __init__(self, n_out=1, szs=[16,64,96,96], ks=3, rnn_ks=5, act=nn.ReLU, blur=False, attn=False,
                 norm=None, debug=False):
        self.n_blocks = len(szs)-1
        self.debug = debug
        deconvs = []
        rnns = []
        szs = szs[::-1]
        rnns.append(ConvGRU_cell(szs[0], szs[0], ks=rnn_ks))
        for ni, nf in zip(szs[0:-2], szs[1:]):
            deconvs.append(UpsampleBlock(ni, ni, blur=blur, self_attention=attn, act_cls=act, norm_type=norm))
            rnns.append(ConvGRU_cell(ni, nf, ks=rnn_ks))

        #last layer
        deconvs.append(ConvLayer(szs[-2], szs[-1], ks, padding=ks//2, act_cls=act, norm_type=norm))
        self.head = TimeDistributed(nn.Conv2d(szs[-1], n_out,kernel_size=1))
        self.deconvs = nn.ModuleList(TimeDistributed(conv) for conv in deconvs)
        self.rnns = nn.ModuleList(rnns)

    def forward_by_stage(self, inputs, state, deconv, rnn):
        if self.debug:
            print(f' Layer: {rnn}')
            print(' inputs:, state: ', inputs.shape, state.shape)
        inputs, state_stage = rnn(inputs, state)
        if self.debug:
            print(' after rnn: ', inputs.shape)
            print(f' Layer: {deconv}')
        outputs_stage = deconv(inputs)
        if self.debug: print(' after_deconvs: ', outputs_stage.shape)
        return outputs_stage, state_stage

    def forward(self, dec_input, hidden_states):
        for i, (state, conv, rnn) in enumerate(zip(hidden_states[::-1], self.deconvs, self.rnns)):
            if self.debug: print('stage: ',i)
            dec_input, state_stage = self.forward_by_stage(dec_input, state, conv, rnn)
        return self.head(dec_input)

# Cell
class StackUnstack(Module):
    "Stack together inputs, apply module, unstack output"
    def __init__(self, module, dim=1):
        self.dim = dim
        self.module = module
    def forward(self, x):
        if isinstance(x, list) or isinstance(x, tuple):
            x = torch.stack(x, dim=self.dim)
            x = self.module(x)
            return x.unbind(dim=self.dim)
        else: return self.module(x)

# Cell
class SimpleModel(Module):
    "Simple Encoder/Decoder module"
    def __init__(self, n_in=1, n_out=1, szs=[16,64,96,96], ks=3, rnn_ks=5, act=nn.ReLU, blur=False, attn=False,
                 norm=None, strategy='zero', debug=False):
        self.strategy = strategy
        self.encoder = Encoder(n_in, szs, ks, rnn_ks, act, norm, debug)
        self.decoder = Decoder(n_out, szs, ks, rnn_ks, act, blur, attn, norm, debug)
    def forward(self, x):
        enc_out, h = self.encoder(x)
        if self.strategy is 'zero':
            dec_in = one_param(self).new_zeros(*enc_out.shape)
        elif self.strategy is 'encoder':
            dec_in = enc_out
        return self.decoder(dec_in, h)

# Cell
def StackLoss(loss_func=MSELossFlat(), axis=1):
    def _inner_loss(x,y):
        x = torch.stack(x, axis)
        y = torch.stack(y, axis)
        return loss_func(x,y)
    return _inner_loss